{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, name, input_size, output_size) -> None:\n",
    "        super(MLP, self).__init__()\n",
    "        self.name = name\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.fc1 = nn.Linear(input_size, 115)  # linear: y = Ax + b\n",
    "        self.fc2 = nn.Linear(115, 78)\n",
    "        self.fc3 = nn.Linear(78, 26)\n",
    "        self.fc4 = nn.Linear(26, 46)\n",
    "        self.fc5 = nn.Linear(46, 82)\n",
    "        self.fc6 = nn.Linear(82, 106)\n",
    "        self.fc7 = nn.Linear(106, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc6(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc7(x)\n",
    "        \n",
    "        output = x = F.relu(x)\n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate fake data (to be replaced with the actual get data functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.random.rand(128, 4)  # 4 dimensional features\n",
    "labels = np.random.rand(128, 5)  # 5 dimensional labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.FloatTensor(features)\n",
    "labels = torch.FloatTensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP('M502', 4, 5)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 930.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.2362939864397049\n",
      "Epoch 1: train loss: 0.23488330841064453\n",
      "Epoch 2: train loss: 0.23493418097496033\n",
      "Epoch 3: train loss: 0.23593325912952423\n",
      "Epoch 4: train loss: 0.23677706718444824\n",
      "Epoch 5: train loss: 0.23673216998577118\n",
      "Epoch 6: train loss: 0.23582465946674347\n",
      "Epoch 7: train loss: 0.2345033586025238\n",
      "Epoch 8: train loss: 0.23320774734020233\n",
      "Epoch 9: train loss: 0.23223650455474854\n",
      "Epoch 10: train loss: 0.23170509934425354\n",
      "Epoch 11: train loss: 0.23157568275928497\n",
      "Epoch 12: train loss: 0.23173141479492188\n",
      "Epoch 13: train loss: 0.23203718662261963\n",
      "Epoch 14: train loss: 0.23237232863903046\n",
      "Epoch 15: train loss: 0.23265290260314941\n",
      "Epoch 16: train loss: 0.23282885551452637\n",
      "Epoch 17: train loss: 0.23288075625896454\n",
      "Epoch 18: train loss: 0.23281314969062805\n",
      "Epoch 19: train loss: 0.23264749348163605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epochs = 20\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    y_pred = model(features)\n",
    "\n",
    "    # compute loss\n",
    "    loss = criterion(y_pred, labels)\n",
    "\n",
    "    print(f'Epoch {epoch}: train loss: {loss.item()}')\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step() # updated = old - learning_rate * gradient\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing your data for training with DataLoaders\n",
    "\n",
    "The `Dataset` retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s `multiprocessing` to speed up data retrieval.\n",
    "\n",
    "`DataLoader` is an iterable that abstracts this complexity for us in an easy API.\n",
    "\n",
    "```\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.TensorDataset(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.053\n",
      "[1,     2] loss: 0.050\n",
      "[1,     3] loss: 0.055\n",
      "[1,     4] loss: 0.061\n",
      "[2,     1] loss: 0.056\n",
      "[2,     2] loss: 0.058\n",
      "[2,     3] loss: 0.055\n",
      "[2,     4] loss: 0.050\n",
      "[3,     1] loss: 0.051\n",
      "[3,     2] loss: 0.058\n",
      "[3,     3] loss: 0.060\n",
      "[3,     4] loss: 0.050\n",
      "[4,     1] loss: 0.056\n",
      "[4,     2] loss: 0.055\n",
      "[4,     3] loss: 0.050\n",
      "[4,     4] loss: 0.058\n",
      "[5,     1] loss: 0.060\n",
      "[5,     2] loss: 0.055\n",
      "[5,     3] loss: 0.055\n",
      "[5,     4] loss: 0.049\n",
      "[6,     1] loss: 0.054\n",
      "[6,     2] loss: 0.053\n",
      "[6,     3] loss: 0.054\n",
      "[6,     4] loss: 0.058\n",
      "[7,     1] loss: 0.056\n",
      "[7,     2] loss: 0.054\n",
      "[7,     3] loss: 0.054\n",
      "[7,     4] loss: 0.054\n",
      "[8,     1] loss: 0.053\n",
      "[8,     2] loss: 0.059\n",
      "[8,     3] loss: 0.054\n",
      "[8,     4] loss: 0.054\n",
      "[9,     1] loss: 0.052\n",
      "[9,     2] loss: 0.055\n",
      "[9,     3] loss: 0.053\n",
      "[9,     4] loss: 0.058\n",
      "[10,     1] loss: 0.053\n",
      "[10,     2] loss: 0.067\n",
      "[10,     3] loss: 0.054\n",
      "[10,     4] loss: 0.044\n",
      "[11,     1] loss: 0.051\n",
      "[11,     2] loss: 0.054\n",
      "[11,     3] loss: 0.060\n",
      "[11,     4] loss: 0.053\n",
      "[12,     1] loss: 0.057\n",
      "[12,     2] loss: 0.052\n",
      "[12,     3] loss: 0.053\n",
      "[12,     4] loss: 0.056\n",
      "[13,     1] loss: 0.049\n",
      "[13,     2] loss: 0.055\n",
      "[13,     3] loss: 0.059\n",
      "[13,     4] loss: 0.055\n",
      "[14,     1] loss: 0.058\n",
      "[14,     2] loss: 0.055\n",
      "[14,     3] loss: 0.055\n",
      "[14,     4] loss: 0.050\n",
      "[15,     1] loss: 0.055\n",
      "[15,     2] loss: 0.055\n",
      "[15,     3] loss: 0.056\n",
      "[15,     4] loss: 0.052\n",
      "[16,     1] loss: 0.057\n",
      "[16,     2] loss: 0.054\n",
      "[16,     3] loss: 0.053\n",
      "[16,     4] loss: 0.054\n",
      "[17,     1] loss: 0.049\n",
      "[17,     2] loss: 0.056\n",
      "[17,     3] loss: 0.055\n",
      "[17,     4] loss: 0.058\n",
      "[18,     1] loss: 0.051\n",
      "[18,     2] loss: 0.056\n",
      "[18,     3] loss: 0.052\n",
      "[18,     4] loss: 0.060\n",
      "[19,     1] loss: 0.049\n",
      "[19,     2] loss: 0.054\n",
      "[19,     3] loss: 0.050\n",
      "[19,     4] loss: 0.051\n",
      "[20,     1] loss: 0.049\n",
      "[20,     2] loss: 0.043\n",
      "[20,     3] loss: 0.041\n",
      "[20,     4] loss: 0.041\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        # if i % 4 == 3:  # print every 4 mini batches (?)\n",
    "        print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss/4:.3f}\\r')\n",
    "        running_loss = 0.0\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "991052e413bb80d7e532c4cba803088624a0d12cdcb58fca553b76858b408112"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
